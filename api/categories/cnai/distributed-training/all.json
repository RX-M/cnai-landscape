[{"category":"CNAI","homepage_url":"https://github.com/alpa-projects/alpa","id":"cnai--distributed-training--apla","logo_url":"http://127.0.0.1:8000/logos/58b98f77df62fd32543239f3890d5a71f3a252c609892c744d7eb329b9f78b43.svg","name":"Apla","subcategory":"Distributed Training"},{"category":"CNAI","homepage_url":"https://github.com/microsoft/DeepSpeed","id":"cnai--distributed-training--deepspeed","logo_url":"http://127.0.0.1:8000/logos/58b98f77df62fd32543239f3890d5a71f3a252c609892c744d7eb329b9f78b43.svg","name":"DeepSpeed","subcategory":"Distributed Training"},{"category":"CNAI","homepage_url":"https://github.com/horovod/horovod","id":"cnai--distributed-training--horovod","logo_url":"http://127.0.0.1:8000/logos/58b98f77df62fd32543239f3890d5a71f3a252c609892c744d7eb329b9f78b43.svg","name":"Horovod","subcategory":"Distributed Training"},{"category":"CNAI","homepage_url":"https://www.kubeflow.org/docs/components/training","id":"cnai--distributed-training--kubeflow-training-operator","logo_url":"http://127.0.0.1:8000/logos/58b98f77df62fd32543239f3890d5a71f3a252c609892c744d7eb329b9f78b43.svg","name":"Kubeflow Training Operator","subcategory":"Distributed Training","description":"Training operators on Kubernetes","repositories":[{"url":"https://github.com/kubeflow/training-operator","primary":true}]},{"category":"CNAI","homepage_url":"https://github.com/NVIDIA/Megatron-LM","id":"cnai--distributed-training--megatron","logo_url":"http://127.0.0.1:8000/logos/58b98f77df62fd32543239f3890d5a71f3a252c609892c744d7eb329b9f78b43.svg","name":"Megatron","subcategory":"Distributed Training"},{"category":"CNAI","homepage_url":"https://www-lb.open-mpi.org/","id":"cnai--distributed-training--open-mpi","logo_url":"http://127.0.0.1:8000/logos/58b98f77df62fd32543239f3890d5a71f3a252c609892c744d7eb329b9f78b43.svg","name":"Open MPI","subcategory":"Distributed Training"},{"category":"CNAI","homepage_url":"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html","id":"cnai--distributed-training--pytorch-distributeddataparallel-ddp","logo_url":"http://127.0.0.1:8000/logos/58b98f77df62fd32543239f3890d5a71f3a252c609892c744d7eb329b9f78b43.svg","name":"Pytorch DistributedDataParallel (DDP)","subcategory":"Distributed Training","description":"DistributedDataParallel (DDP) implements data parallelism at the module level which can run across multiple machines.","repositories":[{"url":"https://github.com/pytorch/pytorch","primary":true}]},{"category":"CNAI","homepage_url":"https://www.tensorflow.org/guide/distributed_training","id":"cnai--distributed-training--tensorflow-distributed","logo_url":"http://127.0.0.1:8000/logos/58b98f77df62fd32543239f3890d5a71f3a252c609892c744d7eb329b9f78b43.svg","name":"Tensorflow Distributed","subcategory":"Distributed Training"},{"category":"CNAI","homepage_url":"https://pytorch.org/torchx/latest","id":"cnai--distributed-training--torchx","logo_url":"http://127.0.0.1:8000/logos/58b98f77df62fd32543239f3890d5a71f3a252c609892c744d7eb329b9f78b43.svg","name":"Torchx","subcategory":"Distributed Training"}]