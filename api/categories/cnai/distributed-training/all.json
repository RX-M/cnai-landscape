[{"category":"CNAI","homepage_url":"https://github.com/alpa-projects/alpa","id":"cnai--distributed-training--apla","logo_url":"http://127.0.0.1:8000/logos/fc8019fa724fbc90c163678c0433d9f3485a79e52e92391ede5899070803d376.svg","name":"Apla","subcategory":"Distributed Training"},{"category":"CNAI","homepage_url":"https://github.com/microsoft/DeepSpeed","id":"cnai--distributed-training--deepspeed","logo_url":"http://127.0.0.1:8000/logos/63e3c2900b14fd489a09efe0c9ef616bfe64fd43c9a2ea69e2d0e7d3ba38bf07.svg","name":"DeepSpeed","subcategory":"Distributed Training","description":"Deep learning optimization library that makes distributed training and inference easy, efficient, and effective.","repositories":[{"url":"https://github.com/microsoft/DeepSpeed","primary":true}]},{"category":"CNAI","homepage_url":"https://horovod.ai/","id":"cnai--distributed-training--horovod","logo_url":"http://127.0.0.1:8000/logos/23c4ae7cfbde29b1dfff309b4c1e2340911b2af8bccafa55e5b1f0a7722a9c4d.svg","name":"Horovod","subcategory":"Distributed Training","description":"Distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.","repositories":[{"url":"https://github.com/horovod/horovod","primary":true}]},{"category":"CNAI","homepage_url":"https://github.com/kubeflow/mpi-operator","id":"cnai--distributed-training--kubeflow-mpi-operator","logo_url":"http://127.0.0.1:8000/logos/7d633b3edfff267c99e5cb38c875ca2b842b3b371fd13dfad598dcc07c24a62b.svg","name":"Kubeflow MPI Operator","subcategory":"Distributed Training","description":"MPI Operator to manage all-reduce distributed training and HPC workloads."},{"category":"CNAI","homepage_url":"https://www.kubeflow.org/docs/components/training","id":"cnai--distributed-training--kubeflow-training-operator","logo_url":"http://127.0.0.1:8000/logos/7d633b3edfff267c99e5cb38c875ca2b842b3b371fd13dfad598dcc07c24a62b.svg","name":"Kubeflow Training Operator","subcategory":"Distributed Training","description":"Training operators on Kubernetes","repositories":[{"url":"https://github.com/kubeflow/training-operator","primary":true}]},{"category":"CNAI","homepage_url":"https://github.com/NVIDIA/Megatron-LM","id":"cnai--distributed-training--megatron-lm","logo_url":"http://127.0.0.1:8000/logos/513bad81db772ece7c84cd78157a3858311bca0edc20582e8dc4c2a38b9260ac.svg","name":"Megatron-LM","subcategory":"Distributed Training","description":"GPU optimized techniques for training transformer models at-scale","repositories":[{"url":"https://github.com/NVIDIA/Megatron-LM","primary":true}]},{"category":"CNAI","homepage_url":"https://www-lb.open-mpi.org/","id":"cnai--distributed-training--open-mpi","logo_url":"http://127.0.0.1:8000/logos/38150482044c5a5a8ad5468c5447b2711599ebc740d1583b4509f760829cd759.svg","name":"Open MPI","subcategory":"Distributed Training"},{"category":"CNAI","homepage_url":"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html","id":"cnai--distributed-training--pytorch-distributeddataparallel-ddp","logo_url":"http://127.0.0.1:8000/logos/5d95234008041810d197e9491aee4df95a9346d3ea8540d794174eadc746274e.svg","name":"Pytorch DistributedDataParallel (DDP)","subcategory":"Distributed Training","description":"DistributedDataParallel (DDP) implements data parallelism at the module level which can run across multiple machines.","repositories":[{"url":"https://github.com/pytorch/pytorch","primary":true}]},{"category":"CNAI","homepage_url":"https://www.tensorflow.org/guide/distributed_training","id":"cnai--distributed-training--tensorflow-distributed","logo_url":"http://127.0.0.1:8000/logos/72f1d3ee2e841763a5dee1442e42a90f2c29322cdbe28125ac561d836b7070c1.svg","name":"Tensorflow Distributed","subcategory":"Distributed Training"},{"category":"CNAI","homepage_url":"https://pytorch.org/torchx/latest","id":"cnai--distributed-training--torchx","logo_url":"http://127.0.0.1:8000/logos/5d95234008041810d197e9491aee4df95a9346d3ea8540d794174eadc746274e.svg","name":"Torchx","subcategory":"Distributed Training"}]